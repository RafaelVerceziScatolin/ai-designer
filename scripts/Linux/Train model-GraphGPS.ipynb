{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6542/3346811239.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(dataset_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs loaded: 1920\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dataset_path = \"/home/rafael/Área de trabalho/Linux/graph_dataset.pt\"\n",
    "dataset = torch.load(dataset_path)\n",
    "\n",
    "print(f\"Graphs loaded: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/anaconda3/envs/dl_torch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/media/rafael/HD/orguel_ml_library\")\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from orguel_ml import BalanceClassWeights, GraphGPSNetwork\n",
    "\n",
    "# Setup\n",
    "epochs = 60\n",
    "batch_size = 1\n",
    "learning_rate = 0.0025\n",
    "weight_decay = 1e-4\n",
    "smoothing_factor = 0.2\n",
    "label_smoothing = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "# split the dataset:\n",
    "train_data, validation_data = train_test_split(dataset, test_size=test_size, shuffle=True, random_state=42)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_weights = BalanceClassWeights(train_data, device, smoothing_factor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "model = GraphGPSNetwork().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01, step_size_up=5, mode=\"triangular\")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-5, threshold=1e-5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.0071 | Train Accuracy: 0.72 | Validation Loss: 0.3747 | Validation Accuracy: 0.87| Learning Rate: 0.002500\n",
      "Epoch 2 | Train Loss: 0.7448 | Train Accuracy: 0.91 | Validation Loss: 0.2724 | Validation Accuracy: 0.92| Learning Rate: 0.002500\n",
      "Epoch 3 | Train Loss: 0.6885 | Train Accuracy: 0.94 | Validation Loss: 0.2217 | Validation Accuracy: 0.95| Learning Rate: 0.002500\n",
      "Epoch 4 | Train Loss: 0.6557 | Train Accuracy: 0.96 | Validation Loss: 0.1858 | Validation Accuracy: 0.97| Learning Rate: 0.002500\n",
      "Epoch 5 | Train Loss: 0.6541 | Train Accuracy: 0.96 | Validation Loss: 0.1702 | Validation Accuracy: 0.97| Learning Rate: 0.002500\n",
      "Epoch 6 | Train Loss: 0.6425 | Train Accuracy: 0.96 | Validation Loss: 0.1946 | Validation Accuracy: 0.96| Learning Rate: 0.002500\n",
      "Epoch 7 | Train Loss: 0.6513 | Train Accuracy: 0.96 | Validation Loss: 0.1528 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 8 | Train Loss: 0.6249 | Train Accuracy: 0.97 | Validation Loss: 0.1916 | Validation Accuracy: 0.97| Learning Rate: 0.002500\n",
      "Epoch 9 | Train Loss: 0.6306 | Train Accuracy: 0.97 | Validation Loss: 0.1486 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 10 | Train Loss: 0.6239 | Train Accuracy: 0.97 | Validation Loss: 0.1543 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 11 | Train Loss: 0.6217 | Train Accuracy: 0.97 | Validation Loss: 0.1455 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 12 | Train Loss: 0.6194 | Train Accuracy: 0.98 | Validation Loss: 0.1547 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 13 | Train Loss: 0.6200 | Train Accuracy: 0.97 | Validation Loss: 0.1557 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 14 | Train Loss: 0.6203 | Train Accuracy: 0.97 | Validation Loss: 0.1441 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 15 | Train Loss: 0.6147 | Train Accuracy: 0.98 | Validation Loss: 0.1632 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 16 | Train Loss: 0.6216 | Train Accuracy: 0.97 | Validation Loss: 0.2181 | Validation Accuracy: 0.96| Learning Rate: 0.002500\n",
      "Epoch 17 | Train Loss: 0.6115 | Train Accuracy: 0.98 | Validation Loss: 0.1504 | Validation Accuracy: 0.97| Learning Rate: 0.002500\n",
      "Epoch 18 | Train Loss: 0.6320 | Train Accuracy: 0.97 | Validation Loss: 0.1393 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 19 | Train Loss: 0.6161 | Train Accuracy: 0.98 | Validation Loss: 0.1462 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 20 | Train Loss: 0.6159 | Train Accuracy: 0.98 | Validation Loss: 0.1305 | Validation Accuracy: 0.99| Learning Rate: 0.002500\n",
      "Epoch 21 | Train Loss: 0.6098 | Train Accuracy: 0.98 | Validation Loss: 0.1529 | Validation Accuracy: 0.98| Learning Rate: 0.002500\n",
      "Epoch 22 | Train Loss: 0.6331 | Train Accuracy: 0.97 | Validation Loss: 0.1371 | Validation Accuracy: 0.99| Learning Rate: 0.002500\n",
      "Epoch 23 | Train Loss: 0.6077 | Train Accuracy: 0.98 | Validation Loss: 0.1372 | Validation Accuracy: 0.99| Learning Rate: 0.002500\n",
      "Epoch 24 | Train Loss: 0.6221 | Train Accuracy: 0.97 | Validation Loss: 0.1355 | Validation Accuracy: 0.99| Learning Rate: 0.001250\n",
      "Epoch 25 | Train Loss: 0.5968 | Train Accuracy: 0.99 | Validation Loss: 0.1245 | Validation Accuracy: 0.99| Learning Rate: 0.001250\n",
      "Epoch 26 | Train Loss: 0.5972 | Train Accuracy: 0.99 | Validation Loss: 0.1250 | Validation Accuracy: 0.99| Learning Rate: 0.001250\n",
      "Epoch 27 | Train Loss: 0.5989 | Train Accuracy: 0.99 | Validation Loss: 0.1957 | Validation Accuracy: 0.96| Learning Rate: 0.001250\n",
      "Epoch 28 | Train Loss: 0.5985 | Train Accuracy: 0.99 | Validation Loss: 0.1280 | Validation Accuracy: 0.99| Learning Rate: 0.001250\n",
      "Epoch 29 | Train Loss: 0.5977 | Train Accuracy: 0.99 | Validation Loss: 0.1205 | Validation Accuracy: 0.99| Learning Rate: 0.001250\n",
      "Epoch 30 | Train Loss: 0.5970 | Train Accuracy: 0.99 | Validation Loss: 0.1228 | Validation Accuracy: 0.99| Learning Rate: 0.001250\n",
      "Epoch 31 | Train Loss: 0.5976 | Train Accuracy: 0.99 | Validation Loss: 0.1277 | Validation Accuracy: 0.99| Learning Rate: 0.001250\n",
      "Epoch 32 | Train Loss: 0.5972 | Train Accuracy: 0.99 | Validation Loss: 0.1245 | Validation Accuracy: 0.99| Learning Rate: 0.001250\n",
      "Epoch 33 | Train Loss: 0.5972 | Train Accuracy: 0.99 | Validation Loss: 0.1377 | Validation Accuracy: 0.99| Learning Rate: 0.000625\n",
      "Epoch 34 | Train Loss: 0.5909 | Train Accuracy: 0.99 | Validation Loss: 0.1185 | Validation Accuracy: 0.99| Learning Rate: 0.000625\n",
      "Epoch 35 | Train Loss: 0.5914 | Train Accuracy: 0.99 | Validation Loss: 0.1171 | Validation Accuracy: 0.99| Learning Rate: 0.000625\n",
      "Epoch 36 | Train Loss: 0.5909 | Train Accuracy: 0.99 | Validation Loss: 0.1165 | Validation Accuracy: 1.00| Learning Rate: 0.000625\n",
      "Epoch 37 | Train Loss: 0.5907 | Train Accuracy: 0.99 | Validation Loss: 0.1164 | Validation Accuracy: 1.00| Learning Rate: 0.000625\n",
      "Epoch 38 | Train Loss: 0.5915 | Train Accuracy: 0.99 | Validation Loss: 0.1168 | Validation Accuracy: 0.99| Learning Rate: 0.000625\n",
      "Epoch 39 | Train Loss: 0.5912 | Train Accuracy: 0.99 | Validation Loss: 0.1181 | Validation Accuracy: 1.00| Learning Rate: 0.000625\n",
      "Epoch 40 | Train Loss: 0.5905 | Train Accuracy: 0.99 | Validation Loss: 0.1170 | Validation Accuracy: 1.00| Learning Rate: 0.000625\n",
      "Epoch 41 | Train Loss: 0.5911 | Train Accuracy: 0.99 | Validation Loss: 0.1187 | Validation Accuracy: 1.00| Learning Rate: 0.000313\n",
      "Epoch 42 | Train Loss: 0.5880 | Train Accuracy: 0.99 | Validation Loss: 0.1142 | Validation Accuracy: 1.00| Learning Rate: 0.000313\n",
      "Epoch 43 | Train Loss: 0.5880 | Train Accuracy: 0.99 | Validation Loss: 0.1135 | Validation Accuracy: 1.00| Learning Rate: 0.000313\n",
      "Epoch 44 | Train Loss: 0.5882 | Train Accuracy: 0.99 | Validation Loss: 0.1146 | Validation Accuracy: 1.00| Learning Rate: 0.000313\n",
      "Epoch 45 | Train Loss: 0.5880 | Train Accuracy: 0.99 | Validation Loss: 0.1155 | Validation Accuracy: 1.00| Learning Rate: 0.000313\n",
      "Epoch 46 | Train Loss: 0.5880 | Train Accuracy: 0.99 | Validation Loss: 0.1153 | Validation Accuracy: 1.00| Learning Rate: 0.000313\n",
      "Epoch 47 | Train Loss: 0.5881 | Train Accuracy: 0.99 | Validation Loss: 0.1152 | Validation Accuracy: 1.00| Learning Rate: 0.000156\n",
      "Epoch 48 | Train Loss: 0.5869 | Train Accuracy: 0.99 | Validation Loss: 0.1115 | Validation Accuracy: 1.00| Learning Rate: 0.000156\n",
      "Epoch 49 | Train Loss: 0.5867 | Train Accuracy: 0.99 | Validation Loss: 0.1126 | Validation Accuracy: 1.00| Learning Rate: 0.000156\n",
      "Epoch 50 | Train Loss: 0.5867 | Train Accuracy: 0.99 | Validation Loss: 0.1140 | Validation Accuracy: 1.00| Learning Rate: 0.000156\n",
      "Epoch 51 | Train Loss: 0.5869 | Train Accuracy: 0.99 | Validation Loss: 0.1127 | Validation Accuracy: 1.00| Learning Rate: 0.000156\n",
      "Epoch 52 | Train Loss: 0.5866 | Train Accuracy: 0.99 | Validation Loss: 0.1140 | Validation Accuracy: 1.00| Learning Rate: 0.000078\n",
      "Epoch 53 | Train Loss: 0.5861 | Train Accuracy: 1.00 | Validation Loss: 0.1111 | Validation Accuracy: 1.00| Learning Rate: 0.000078\n",
      "Epoch 54 | Train Loss: 0.5860 | Train Accuracy: 1.00 | Validation Loss: 0.1115 | Validation Accuracy: 1.00| Learning Rate: 0.000078\n",
      "Epoch 55 | Train Loss: 0.5862 | Train Accuracy: 1.00 | Validation Loss: 0.1126 | Validation Accuracy: 1.00| Learning Rate: 0.000078\n",
      "Epoch 56 | Train Loss: 0.5860 | Train Accuracy: 1.00 | Validation Loss: 0.1114 | Validation Accuracy: 1.00| Learning Rate: 0.000078\n",
      "Epoch 57 | Train Loss: 0.5860 | Train Accuracy: 1.00 | Validation Loss: 0.1117 | Validation Accuracy: 1.00| Learning Rate: 0.000039\n",
      "Epoch 58 | Train Loss: 0.5858 | Train Accuracy: 1.00 | Validation Loss: 0.1109 | Validation Accuracy: 1.00| Learning Rate: 0.000039\n",
      "Epoch 59 | Train Loss: 0.5859 | Train Accuracy: 1.00 | Validation Loss: 0.1106 | Validation Accuracy: 1.00| Learning Rate: 0.000039\n",
      "Epoch 60 | Train Loss: 0.5858 | Train Accuracy: 1.00 | Validation Loss: 0.1108 | Validation Accuracy: 1.00| Learning Rate: 0.000039\n",
      "\n",
      "Training complete. You can now launch TensorBoard:\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"TensorBoard\")\n",
    "\n",
    "# Training loop with Cross Entropy clearly shown\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    acumulateLoss = 0\n",
    "    correctPredictions = 0\n",
    "    totalNodesProcessed = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        \n",
    "        # cross_entropy expects raw logits:\n",
    "        loss = F.cross_entropy(output, batch.y, weight=class_weights, label_smoothing=label_smoothing)\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step() # CyclicLR\n",
    "        \n",
    "        # compute accuracy\n",
    "        acumulateLoss += loss.item()\n",
    "        predictedClass = output.argmax(dim=1)\n",
    "        correctPredictions += (predictedClass == batch.y).sum().item()\n",
    "        totalNodesProcessed += batch.num_nodes\n",
    "    \n",
    "    # Computes epoch-wide accuracy & loss\n",
    "    trainAccuracy = correctPredictions / totalNodesProcessed\n",
    "    averageTrainLoss = acumulateLoss / len(train_loader)\n",
    "    \n",
    "    # Evaluate clearly:\n",
    "    model.eval()\n",
    "    acumulateLoss = 0\n",
    "    correctPredictions = 0\n",
    "    totalNodesProcessed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            batch = batch.to(device)\n",
    "            output = model(batch)\n",
    "            loss = F.cross_entropy(output, batch.y, weight=class_weights)\n",
    "            acumulateLoss += loss.item()\n",
    "            predictedClass = output.argmax(dim=1)\n",
    "            correctPredictions += (predictedClass == batch.y).sum().item()\n",
    "            totalNodesProcessed += batch.num_nodes\n",
    "\n",
    "    ValidationAccuracy = correctPredictions / totalNodesProcessed\n",
    "    averageValidationLoss = acumulateLoss / len(validation_loader)\n",
    "    \n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(averageValidationLoss) # ReduceLROnPlateau\n",
    "    \n",
    "    # Log learning rate\n",
    "    currentLearningRate = optimizer.param_groups[0]['lr']\n",
    "    writer.add_scalar(\"LearningRate\", currentLearningRate, epoch)\n",
    "    \n",
    "    # Logging\n",
    "    writer.add_scalar(\"Loss/train\", averageTrainLoss, epoch)\n",
    "    writer.add_scalar(\"Loss/val\", averageValidationLoss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", trainAccuracy, epoch)\n",
    "    writer.add_scalar(\"Accuracy/val\", ValidationAccuracy, epoch)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {averageTrainLoss:.4f} | Train Accuracy: {trainAccuracy:.2f} | Validation Loss: {averageValidationLoss:.4f} | Validation Accuracy: {ValidationAccuracy:.2f}| Learning Rate: {currentLearningRate:.6f}\")\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"\\nTraining complete. You can now launch TensorBoard:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/rafael/Área de trabalho/Linux/GraphGPSNetwork.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model to a file\n",
    "save_path = \"/home/rafael/Área de trabalho/Linux/GraphGPSNetwork.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 152448, 1: 133504, 3: 130224, 2: 7648})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# number of labels of each class\n",
    "labels = [data.y.tolist() for data in dataset]\n",
    "flat_labels = [item for sublist in labels for item in sublist]\n",
    "print(Counter(flat_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
