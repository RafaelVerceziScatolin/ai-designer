{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9446/3346811239.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(dataset_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs loaded: 620\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dataset_path = \"/home/rafael/Área de trabalho/Linux/graph_dataset.pt\"\n",
    "dataset = torch.load(dataset_path)\n",
    "\n",
    "print(f\"Graphs loaded: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/media/rafael/HD/orguel_ml_library\")\n",
    "from orguel_ml import BalanceClassWeights\n",
    "from orguel_ml.ocr import OCRNetwork, character_set\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "learning_rate = 0.005\n",
    "weight_decay = 1e-4\n",
    "smoothing_factor = 0.2\n",
    "label_smoothing = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "# split the dataset:\n",
    "train_data, validation_data = train_test_split(dataset, test_size=test_size, shuffle=True, random_state=42)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Character and font class weights\n",
    "n_characters = len(character_set[\"characters\"][\"encoding\"])\n",
    "n_fonts = len(character_set[\"font\"][\"encoding\"])\n",
    "character_weights = BalanceClassWeights(train_data, device, smoothing_factor, classification_labels=n_characters)\n",
    "font_weights = BalanceClassWeights(train_data, device, smoothing_factor, classification_labels=n_fonts)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "model = OCRNetwork(cluster_aware=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01, step_size_up=5, mode=\"triangular\")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-5, threshold=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Total Train Loss: 2.0501 | Train Accuracy Character: 0.84 | Total Validation Loss: 1.0239 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 2 | Total Train Loss: 1.3707 | Train Accuracy Character: 0.96 | Total Validation Loss: 0.9867 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 3 | Total Train Loss: 1.3324 | Train Accuracy Character: 0.96 | Total Validation Loss: 1.0264 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 4 | Total Train Loss: 1.3720 | Train Accuracy Character: 0.96 | Total Validation Loss: 1.2261 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 5 | Total Train Loss: 1.3486 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9601 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 6 | Total Train Loss: 1.2914 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9545 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 7 | Total Train Loss: 1.2873 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9369 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 8 | Total Train Loss: 1.2823 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9366 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 9 | Total Train Loss: 1.3568 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9438 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 10 | Total Train Loss: 1.2755 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9388 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 11 | Total Train Loss: 1.2681 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9214 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 12 | Total Train Loss: 1.2657 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9413 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 13 | Total Train Loss: 1.2653 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9369 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 14 | Total Train Loss: 1.2700 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9429 | Validation Accuracy Character: 1.00| Learning Rate: 0.005000\n",
      "Epoch 15 | Total Train Loss: 1.2690 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9267 | Validation Accuracy Character: 1.00| Learning Rate: 0.002500\n",
      "Epoch 16 | Total Train Loss: 1.2491 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9232 | Validation Accuracy Character: 1.00| Learning Rate: 0.002500\n",
      "Epoch 17 | Total Train Loss: 1.2480 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9166 | Validation Accuracy Character: 1.00| Learning Rate: 0.002500\n",
      "Epoch 18 | Total Train Loss: 1.2492 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9232 | Validation Accuracy Character: 1.00| Learning Rate: 0.002500\n",
      "Epoch 19 | Total Train Loss: 1.2483 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9185 | Validation Accuracy Character: 1.00| Learning Rate: 0.002500\n",
      "Epoch 20 | Total Train Loss: 1.2489 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9192 | Validation Accuracy Character: 1.00| Learning Rate: 0.002500\n",
      "Epoch 21 | Total Train Loss: 1.2494 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9219 | Validation Accuracy Character: 1.00| Learning Rate: 0.001250\n",
      "Epoch 22 | Total Train Loss: 1.2443 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9171 | Validation Accuracy Character: 1.00| Learning Rate: 0.001250\n",
      "Epoch 23 | Total Train Loss: 1.2432 | Train Accuracy Character: 0.97 | Total Validation Loss: 0.9159 | Validation Accuracy Character: 1.00| Learning Rate: 0.001250\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"TensorBoard\")\n",
    "\n",
    "# Training loop with Cross Entropy clearly shown\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    acumulatedTotalLoss = 0\n",
    "    acumulatedCharacterLoss, acumulatedFontLoss = 0, 0\n",
    "    acumulatedHeightLoss, acumulatedRotationLoss, acumulatedInsertionLoss = 0, 0, 0\n",
    "    acumulatedMAEHeight, acumulatedMAERotation, acumulatedMAEInsertion = 0, 0, 0\n",
    "    acumulatedRMSEHeight, acumulatedRMSERotation, acumulatedRMSEInsertion = 0, 0, 0\n",
    "    correctPredictedCharacter, totalCharactersProcessed = 0, 0\n",
    "    correctPredictedFont, totalFontsProcessed = 0, 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        \n",
    "        height = output[\"regression_values\"][:, 0]\n",
    "        rotation = output[\"regression_values\"][:, 1]\n",
    "        insertion_x = output[\"regression_values\"][:, 2]\n",
    "        insertion_y = output[\"regression_values\"][:, 3]\n",
    "        \n",
    "        characterLoss = F.cross_entropy(output[\"character_logits\"], batch.y, weight=character_weights, label_smoothing=label_smoothing)\n",
    "        fontLoss = F.cross_entropy(output[\"font_logits\"], batch.font, weight=font_weights)\n",
    "        heightLoss = F.mse_loss(height, batch.height.squeeze(-1))\n",
    "        rotationLoss = F.mse_loss(rotation, batch.rotation.squeeze(-1))\n",
    "        insertionLoss = F.mse_loss(insertion_x, batch.insertion_x.squeeze(-1)) + F.mse_loss(insertion_y, batch.insertion_y.squeeze(-1))\n",
    "        \n",
    "        totalLoss = characterLoss + 0.5*fontLoss + 0.5*(heightLoss + rotationLoss + insertionLoss)\n",
    "        \n",
    "        # backpropagation\n",
    "        totalLoss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step() # CyclicLR\n",
    "        \n",
    "        # compute accuracy\n",
    "        predictedCharacter = output[\"character_logits\"].argmax(dim=1)\n",
    "        predictedFont = output[\"font_logits\"].argmax(dim=1)\n",
    "        \n",
    "        acumulatedTotalLoss += totalLoss.item()\n",
    "        acumulatedCharacterLoss += characterLoss.item()\n",
    "        acumulatedFontLoss += fontLoss.item()\n",
    "        acumulatedHeightLoss += heightLoss.item()\n",
    "        acumulatedRotationLoss += rotationLoss.item()\n",
    "        acumulatedInsertionLoss += insertionLoss.item()\n",
    "        \n",
    "        correctPredictedCharacter += (predictedCharacter == batch.y).sum().item()\n",
    "        correctPredictedFont += (predictedFont == batch.font).sum().item()\n",
    "        \n",
    "        totalCharactersProcessed += batch.y.size(0)\n",
    "        totalFontsProcessed += batch.font.size(0)\n",
    "        \n",
    "        # MAE and RMSE\n",
    "        predictedInsertion = torch.stack([insertion_x, insertion_y], dim=1)\n",
    "        correctPredictedInsertion = torch.stack([batch.insertion_x.squeeze(-1), batch.insertion_y.squeeze(-1)], dim=1)\n",
    "        acumulatedMAEHeight += F.l1_loss(height, batch.height.squeeze(-1)).item()\n",
    "        acumulatedMAERotation += F.l1_loss(rotation, batch.rotation.squeeze(-1)).item()\n",
    "        acumulatedMAEInsertion += F.l1_loss(predictedInsertion, correctPredictedInsertion).item()\n",
    "        acumulatedRMSEHeight += torch.sqrt(F.mse_loss(height, batch.height.squeeze(-1))).item()\n",
    "        acumulatedRMSERotation += torch.sqrt(F.mse_loss(rotation, batch.rotation.squeeze(-1))).item()\n",
    "        acumulatedRMSEInsertion += torch.sqrt(F.mse_loss(predictedInsertion, correctPredictedInsertion)).item()\n",
    "    \n",
    "    # Computes epoch-wide accuracy & loss\n",
    "    trainAccuracyCharacter = correctPredictedCharacter / totalCharactersProcessed\n",
    "    trainAccuracyFont = correctPredictedFont / totalFontsProcessed\n",
    "    \n",
    "    averageTrainLossTotal = acumulatedTotalLoss / len(train_loader)\n",
    "    averageTrainLossCharacter = acumulatedCharacterLoss / len(train_loader)\n",
    "    averageTrainLossFont = acumulatedFontLoss / len(train_loader)\n",
    "    averageTrainLossHeight = acumulatedHeightLoss / len(train_loader)\n",
    "    averageTrainLossRotation = acumulatedRotationLoss / len(train_loader)\n",
    "    averageTrainLossInsertion = acumulatedInsertionLoss / len(train_loader)\n",
    "    \n",
    "    # Evaluate clearly:\n",
    "    model.eval()\n",
    "    acumulatedTotalLoss = 0\n",
    "    acumulatedCharacterLoss, acumulatedFontLoss = 0, 0\n",
    "    acumulatedHeightLoss, acumulatedRotationLoss, acumulatedInsertionLoss = 0, 0, 0\n",
    "    acumulatedMAEHeight, acumulatedMAERotation, acumulatedMAEInsertion = 0, 0, 0\n",
    "    acumulatedRMSEHeight, acumulatedRMSERotation, acumulatedRMSEInsertion = 0, 0, 0\n",
    "    correctPredictedCharacter, totalCharactersProcessed = 0, 0\n",
    "    correctPredictedFont, totalFontsProcessed = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            batch = batch.to(device)\n",
    "            output = model(batch)\n",
    "            \n",
    "            height = output[\"regression_values\"][:, 0]\n",
    "            rotation = output[\"regression_values\"][:, 1]\n",
    "            insertion_x = output[\"regression_values\"][:, 2]\n",
    "            insertion_y = output[\"regression_values\"][:, 3]\n",
    "            \n",
    "            characterLoss = F.cross_entropy(output[\"character_logits\"], batch.y, weight=character_weights, label_smoothing=label_smoothing)\n",
    "            fontLoss = F.cross_entropy(output[\"font_logits\"], batch.font, weight=font_weights)\n",
    "            heightLoss = F.mse_loss(height, batch.height.squeeze(-1))\n",
    "            rotationLoss = F.mse_loss(rotation, batch.rotation.squeeze(-1))\n",
    "            insertionLoss = F.mse_loss(insertion_x, batch.insertion_x.squeeze(-1)) + F.mse_loss(insertion_y, batch.insertion_y.squeeze(-1))\n",
    "            \n",
    "            totalLoss = characterLoss + 0.5*fontLoss + 0.5*(heightLoss + rotationLoss + insertionLoss)\n",
    "            \n",
    "            # compute accuracy\n",
    "            predictedCharacter = output[\"character_logits\"].argmax(dim=1)\n",
    "            predictedFont = output[\"font_logits\"].argmax(dim=1)\n",
    "            \n",
    "            acumulatedTotalLoss += totalLoss.item()\n",
    "            acumulatedCharacterLoss += characterLoss.item()\n",
    "            acumulatedFontLoss += fontLoss.item()\n",
    "            acumulatedHeightLoss += heightLoss.item()\n",
    "            acumulatedRotationLoss += rotationLoss.item()\n",
    "            acumulatedInsertionLoss += insertionLoss.item()\n",
    "            \n",
    "            correctPredictedCharacter += (predictedCharacter == batch.y).sum().item()\n",
    "            correctPredictedFont += (predictedFont == batch.font).sum().item()\n",
    "            \n",
    "            totalCharactersProcessed += batch.y.size(0)\n",
    "            totalFontsProcessed += batch.font.size(0)\n",
    "            \n",
    "            # MAE and RMSE\n",
    "            predictedInsertion = torch.stack([insertion_x, insertion_y], dim=1)\n",
    "            correctPredictedInsertion = torch.stack([batch.insertion_x.squeeze(-1), batch.insertion_y.squeeze(-1)], dim=1)\n",
    "            acumulatedMAEHeight += F.l1_loss(height, batch.height.squeeze(-1)).item()\n",
    "            acumulatedMAERotation += F.l1_loss(rotation, batch.rotation.squeeze(-1)).item()\n",
    "            acumulatedMAEInsertion += F.l1_loss(predictedInsertion, correctPredictedInsertion).item()\n",
    "\n",
    "            acumulatedRMSEHeight += torch.sqrt(F.mse_loss(height, batch.height.squeeze(-1))).item()\n",
    "            acumulatedRMSERotation += torch.sqrt(F.mse_loss(rotation, batch.rotation.squeeze(-1))).item()\n",
    "            acumulatedRMSEInsertion += torch.sqrt(F.mse_loss(predictedInsertion, correctPredictedInsertion)).item()        \n",
    "                        \n",
    "    # Computes epoch-wide accuracy & loss\n",
    "    validationAccuracyCharacter = correctPredictedCharacter / totalCharactersProcessed\n",
    "    validationAccuracyFont = correctPredictedFont / totalFontsProcessed\n",
    "    \n",
    "    averageValidationLossTotal = acumulatedTotalLoss / len(validation_loader)\n",
    "    averageValidationLossCharacter = acumulatedCharacterLoss / len(validation_loader)\n",
    "    averageValidationLossFont = acumulatedFontLoss / len(validation_loader)\n",
    "    averageValidationLossHeight = acumulatedHeightLoss / len(validation_loader)\n",
    "    averageValidationLossRotation = acumulatedRotationLoss / len(validation_loader)\n",
    "    averageValidationLossInsertion = acumulatedInsertionLoss / len(validation_loader)\n",
    "    \n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(averageValidationLossTotal) # ReduceLROnPlateau\n",
    "    \n",
    "    # Logging\n",
    "    currentLearningRate = optimizer.param_groups[0]['lr']\n",
    "    writer.add_scalar(\"LearningRate\", currentLearningRate, epoch)\n",
    "    \n",
    "    writer.add_scalar(\"Accuracy/train_character\", trainAccuracyCharacter, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train_font\", trainAccuracyFont, epoch)\n",
    "    \n",
    "    writer.add_scalar(\"Accuracy/validation_character\", validationAccuracyCharacter, epoch)\n",
    "    writer.add_scalar(\"Accuracy/validation_font\", validationAccuracyFont, epoch)\n",
    "    \n",
    "    writer.add_scalar(\"Loss/train_total\", averageTrainLossTotal, epoch)\n",
    "    writer.add_scalar(\"Loss/train_character\", averageTrainLossCharacter, epoch)\n",
    "    writer.add_scalar(\"Loss/train_font\", averageTrainLossFont, epoch)\n",
    "    writer.add_scalar(\"Loss/train_height\", averageTrainLossHeight, epoch)\n",
    "    writer.add_scalar(\"Loss/train_rotation\", averageTrainLossRotation, epoch)\n",
    "    writer.add_scalar(\"Loss/train_insertion_point\", averageTrainLossInsertion, epoch)\n",
    "    \n",
    "    writer.add_scalar(\"Loss/validation_total\", averageValidationLossTotal, epoch)\n",
    "    writer.add_scalar(\"Loss/validation_character\", averageValidationLossCharacter, epoch)\n",
    "    writer.add_scalar(\"Loss/validation_font\", averageValidationLossFont, epoch)\n",
    "    writer.add_scalar(\"Loss/validation_height\", averageValidationLossHeight, epoch)\n",
    "    writer.add_scalar(\"Loss/validation_rotation\", averageValidationLossRotation, epoch)\n",
    "    writer.add_scalar(\"Loss/validation_insertion_point\", averageValidationLossInsertion, epoch)\n",
    "    \n",
    "    writer.add_scalar(\"MAE/train_height\", acumulatedMAEHeight / len(train_loader), epoch)\n",
    "    writer.add_scalar(\"MAE/train_rotation\", acumulatedMAERotation / len(train_loader), epoch)\n",
    "    writer.add_scalar(\"MAE/train_insertion_point\", acumulatedMAEInsertion / len(train_loader), epoch)\n",
    "    writer.add_scalar(\"RMSE/train_height\", acumulatedRMSEHeight / len(train_loader), epoch)\n",
    "    writer.add_scalar(\"RMSE/train_rotation\", acumulatedRMSERotation / len(train_loader), epoch)\n",
    "    writer.add_scalar(\"RMSE/train_insertion_point\", acumulatedRMSEInsertion / len(train_loader), epoch)\n",
    "    \n",
    "    writer.add_scalar(\"MAE/validation_height\", acumulatedMAEHeight / len(validation_loader), epoch)\n",
    "    writer.add_scalar(\"MAE/validation_rotation\", acumulatedMAERotation / len(validation_loader), epoch)\n",
    "    writer.add_scalar(\"MAE/validation_insertion_point\", acumulatedMAEInsertion / len(validation_loader), epoch)\n",
    "    writer.add_scalar(\"RMSE/validation_height\", acumulatedRMSEHeight / len(validation_loader), epoch)\n",
    "    writer.add_scalar(\"RMSE/validation_rotation\", acumulatedRMSERotation / len(validation_loader), epoch)\n",
    "    writer.add_scalar(\"RMSE/validation_insertion_point\", acumulatedRMSEInsertion / len(validation_loader), epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Total Train Loss: {averageTrainLossTotal:.4f} | Train Accuracy Character: {trainAccuracyCharacter:.2f} | Total Validation Loss: {averageValidationLossTotal:.4f} | Validation Accuracy Character: {validationAccuracyCharacter:.2f}| Learning Rate: {currentLearningRate:.6f}\")\n",
    "    \n",
    "writer.close()\n",
    "\n",
    "print(\"\\nTraining complete. You can now launch TensorBoard:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/rafael/Área de trabalho/Linux/OCRNetwork.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model to a file\n",
    "save_path = \"/home/rafael/Área de trabalho/Linux/OCRNetwork.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
