{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29276/3722063709.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(dataset_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs loaded: 4320 / OS: Linux / device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from platform import system\n",
    "sys.path.append(Path.home() / \"orguel_ml\")\n",
    "\n",
    "system = system()\n",
    "\n",
    "if system == 'Windows': dataset_path = \"D:\\\\ml\\\\graph_dataset.pt\"; save_path = \"D:\\\\ml\\\\GNN.pt\"\n",
    "elif system == 'Linux': dataset_path = \"/media/rafael/HD/ml/graph_dataset.pt\"; save_path = \"/media/rafael/HD/ml/GNN.pt\"\n",
    "\n",
    "dataset = torch.load(dataset_path)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(f\"Graphs loaded: {len(dataset)} / OS: {system} / device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e97ef13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label counts: {0: 492967, 1: 582148, 2: 21154, 3: 264305, 4: 46371, 5: 98163, 6: 695629}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/anaconda3/envs/dl_torch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import orguel_ml\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Setup\n",
    "test_size = 0.1\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "n_targets = 7\n",
    "smoothing_exp = 0.2\n",
    "label_smoothing = 0.1\n",
    "epochs = 60\n",
    "\n",
    "# split the dataset:\n",
    "training_dataset, validation_dataset = train_test_split(dataset, test_size=test_size, shuffle=True, random_state=42)\n",
    "\n",
    "# DataLoaders\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# degree histogram from *training* graphs only\n",
    "pna_degree = orguel_ml.compute_pna_degree(training_dataset)\n",
    "model = orguel_ml.GraphNeuralNetwork(pna_degree).to(device)\n",
    "\n",
    "# Optimizer, scheduler, loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-5, threshold=1e-5, verbose=True)\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0025, max_lr=0.005, step_size_up=5, mode=\"triangular\")\n",
    "\n",
    "class_weights, label_counts = orguel_ml.balance_class_weights(training_dataset, n_targets, smoothing_exp, device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"TensorBoard\")\n",
    "\n",
    "# Train/Eval loops\n",
    "_is_device_cuda = True if device.type=='cuda' else False\n",
    "scaler = torch.amp.GradScaler(device.type, enabled=_is_device_cuda) # mixed precision for speed\n",
    "\n",
    "def run_epoch(loader, training=False):\n",
    "    \n",
    "    model.train() if training else model.eval()\n",
    "    \n",
    "    n_batches = len(loader)\n",
    "    total_loss, correct, total_nodes = 0., 0, 0\n",
    "    \n",
    "    with torch.enable_grad() if training else torch.inference_mode():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            with torch.amp.autocast(device.type, enabled=_is_device_cuda):\n",
    "                logits = model(batch); loss = criterion(logits, batch.y)\n",
    "            \n",
    "            if training:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                #scheduler.step() # CyclicLR\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted = logits.argmax(dim=1)\n",
    "            correct += (predicted==batch.y).sum().item()\n",
    "            total_nodes += batch.num_nodes\n",
    "    \n",
    "    average_loss = total_loss / n_batches\n",
    "    accuracy = correct / total_nodes\n",
    "    \n",
    "    return average_loss, accuracy\n",
    "\n",
    "print(f\"label counts: {dict(sorted(label_counts.items()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf28039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | TRAINING   Loss: 0.8831   Accuracy: 0.862 | VALIDATION   Loss: 0.6202   Accuracy: 0.972 | LR 0.001000\n",
      "Epoch 002 | TRAINING   Loss: 0.6139   Accuracy: 0.983 | VALIDATION   Loss: 0.5676   Accuracy: 0.993 | LR 0.001000\n",
      "Epoch 003 | TRAINING   Loss: 0.5858   Accuracy: 0.993 | VALIDATION   Loss: 0.5571   Accuracy: 0.996 | LR 0.001000\n",
      "Epoch 004 | TRAINING   Loss: 0.5773   Accuracy: 0.995 | VALIDATION   Loss: 0.5548   Accuracy: 0.997 | LR 0.001000\n",
      "Epoch 005 | TRAINING   Loss: 0.5733   Accuracy: 0.996 | VALIDATION   Loss: 0.5561   Accuracy: 0.997 | LR 0.001000\n",
      "Epoch 006 | TRAINING   Loss: 0.5756   Accuracy: 0.995 | VALIDATION   Loss: 0.5567   Accuracy: 0.996 | LR 0.001000\n",
      "Epoch 007 | TRAINING   Loss: 0.5703   Accuracy: 0.996 | VALIDATION   Loss: 0.5514   Accuracy: 0.997 | LR 0.001000\n",
      "Epoch 008 | TRAINING   Loss: 0.5683   Accuracy: 0.997 | VALIDATION   Loss: 0.5513   Accuracy: 0.997 | LR 0.001000\n",
      "Epoch 009 | TRAINING   Loss: 0.5669   Accuracy: 0.997 | VALIDATION   Loss: 0.5511   Accuracy: 0.997 | LR 0.001000\n",
      "Epoch 010 | TRAINING   Loss: 0.5740   Accuracy: 0.995 | VALIDATION   Loss: 0.5727   Accuracy: 0.991 | LR 0.001000\n",
      "Epoch 011 | TRAINING   Loss: 0.5691   Accuracy: 0.996 | VALIDATION   Loss: 0.5504   Accuracy: 0.997 | LR 0.001000\n",
      "Epoch 012 | TRAINING   Loss: 0.5642   Accuracy: 0.997 | VALIDATION   Loss: 0.5500   Accuracy: 0.998 | LR 0.001000\n",
      "Epoch 013 | TRAINING   Loss: 0.5644   Accuracy: 0.997 | VALIDATION   Loss: 0.5509   Accuracy: 0.997 | LR 0.001000\n",
      "Epoch 014 | TRAINING   Loss: 0.5653   Accuracy: 0.997 | VALIDATION   Loss: 0.5514   Accuracy: 0.997 | LR 0.001000\n",
      "Epoch 015 | TRAINING   Loss: 0.5639   Accuracy: 0.997 | VALIDATION   Loss: 0.5508   Accuracy: 0.997 | LR 0.001000\n",
      "Epoch 016 | TRAINING   Loss: 0.5623   Accuracy: 0.997 | VALIDATION   Loss: 0.5512   Accuracy: 0.997 | LR 0.000500\n",
      "Epoch 017 | TRAINING   Loss: 0.5614   Accuracy: 0.998 | VALIDATION   Loss: 0.5486   Accuracy: 0.998 | LR 0.000500\n",
      "Epoch 018 | TRAINING   Loss: 0.5606   Accuracy: 0.998 | VALIDATION   Loss: 0.5482   Accuracy: 0.998 | LR 0.000500\n",
      "Epoch 019 | TRAINING   Loss: 0.5608   Accuracy: 0.998 | VALIDATION   Loss: 0.5486   Accuracy: 0.998 | LR 0.000500\n",
      "Epoch 020 | TRAINING   Loss: 0.5604   Accuracy: 0.998 | VALIDATION   Loss: 0.5484   Accuracy: 0.998 | LR 0.000500\n",
      "Epoch 021 | TRAINING   Loss: 0.5599   Accuracy: 0.998 | VALIDATION   Loss: 0.5483   Accuracy: 0.998 | LR 0.000500\n",
      "Epoch 022 | TRAINING   Loss: 0.5602   Accuracy: 0.998 | VALIDATION   Loss: 0.5490   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 023 | TRAINING   Loss: 0.5595   Accuracy: 0.998 | VALIDATION   Loss: 0.5476   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 024 | TRAINING   Loss: 0.5592   Accuracy: 0.998 | VALIDATION   Loss: 0.5476   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 025 | TRAINING   Loss: 0.5590   Accuracy: 0.998 | VALIDATION   Loss: 0.5477   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 026 | TRAINING   Loss: 0.5593   Accuracy: 0.998 | VALIDATION   Loss: 0.5481   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 027 | TRAINING   Loss: 0.5588   Accuracy: 0.998 | VALIDATION   Loss: 0.5479   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 028 | TRAINING   Loss: 0.5592   Accuracy: 0.998 | VALIDATION   Loss: 0.5473   Accuracy: 0.999 | LR 0.000250\n",
      "Epoch 029 | TRAINING   Loss: 0.5588   Accuracy: 0.998 | VALIDATION   Loss: 0.5475   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 030 | TRAINING   Loss: 0.5587   Accuracy: 0.998 | VALIDATION   Loss: 0.5471   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 031 | TRAINING   Loss: 0.5585   Accuracy: 0.998 | VALIDATION   Loss: 0.5471   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 032 | TRAINING   Loss: 0.5586   Accuracy: 0.998 | VALIDATION   Loss: 0.5474   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 033 | TRAINING   Loss: 0.5584   Accuracy: 0.998 | VALIDATION   Loss: 0.5476   Accuracy: 0.998 | LR 0.000250\n",
      "Epoch 034 | TRAINING   Loss: 0.5585   Accuracy: 0.998 | VALIDATION   Loss: 0.5483   Accuracy: 0.998 | LR 0.000125\n",
      "Epoch 035 | TRAINING   Loss: 0.5580   Accuracy: 0.998 | VALIDATION   Loss: 0.5470   Accuracy: 0.999 | LR 0.000125\n",
      "Epoch 036 | TRAINING   Loss: 0.5578   Accuracy: 0.999 | VALIDATION   Loss: 0.5475   Accuracy: 0.998 | LR 0.000125\n",
      "Epoch 037 | TRAINING   Loss: 0.5578   Accuracy: 0.999 | VALIDATION   Loss: 0.5470   Accuracy: 0.999 | LR 0.000125\n",
      "Epoch 038 | TRAINING   Loss: 0.5576   Accuracy: 0.999 | VALIDATION   Loss: 0.5471   Accuracy: 0.998 | LR 0.000125\n",
      "Epoch 039 | TRAINING   Loss: 0.5574   Accuracy: 0.999 | VALIDATION   Loss: 0.5467   Accuracy: 0.999 | LR 0.000125\n",
      "Epoch 040 | TRAINING   Loss: 0.5574   Accuracy: 0.999 | VALIDATION   Loss: 0.5470   Accuracy: 0.999 | LR 0.000125\n",
      "Epoch 041 | TRAINING   Loss: 0.5574   Accuracy: 0.999 | VALIDATION   Loss: 0.5467   Accuracy: 0.999 | LR 0.000125\n",
      "Epoch 042 | TRAINING   Loss: 0.5575   Accuracy: 0.999 | VALIDATION   Loss: 0.5468   Accuracy: 0.999 | LR 0.000125\n",
      "Epoch 043 | TRAINING   Loss: 0.5572   Accuracy: 0.999 | VALIDATION   Loss: 0.5470   Accuracy: 0.999 | LR 0.000125\n",
      "Epoch 044 | TRAINING   Loss: 0.5573   Accuracy: 0.999 | VALIDATION   Loss: 0.5467   Accuracy: 0.999 | LR 0.000125\n",
      "Epoch 045 | TRAINING   Loss: 0.5575   Accuracy: 0.999 | VALIDATION   Loss: 0.5472   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 046 | TRAINING   Loss: 0.5571   Accuracy: 0.999 | VALIDATION   Loss: 0.5467   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 047 | TRAINING   Loss: 0.5569   Accuracy: 0.999 | VALIDATION   Loss: 0.5467   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 048 | TRAINING   Loss: 0.5570   Accuracy: 0.999 | VALIDATION   Loss: 0.5468   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 049 | TRAINING   Loss: 0.5569   Accuracy: 0.999 | VALIDATION   Loss: 0.5467   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 050 | TRAINING   Loss: 0.5566   Accuracy: 0.999 | VALIDATION   Loss: 0.5466   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 051 | TRAINING   Loss: 0.5567   Accuracy: 0.999 | VALIDATION   Loss: 0.5464   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 052 | TRAINING   Loss: 0.5568   Accuracy: 0.999 | VALIDATION   Loss: 0.5465   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 053 | TRAINING   Loss: 0.5568   Accuracy: 0.999 | VALIDATION   Loss: 0.5468   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 054 | TRAINING   Loss: 0.5567   Accuracy: 0.999 | VALIDATION   Loss: 0.5471   Accuracy: 0.999 | LR 0.000063\n",
      "Epoch 055 | TRAINING   Loss: 0.5568   Accuracy: 0.999 | VALIDATION   Loss: 0.5465   Accuracy: 0.999 | LR 0.000031\n",
      "Epoch 056 | TRAINING   Loss: 0.5566   Accuracy: 0.999 | VALIDATION   Loss: 0.5465   Accuracy: 0.999 | LR 0.000031\n",
      "Epoch 057 | TRAINING   Loss: 0.5566   Accuracy: 0.999 | VALIDATION   Loss: 0.5465   Accuracy: 0.999 | LR 0.000031\n",
      "Epoch 058 | TRAINING   Loss: 0.5565   Accuracy: 0.999 | VALIDATION   Loss: 0.5464   Accuracy: 0.999 | LR 0.000031\n",
      "Epoch 059 | TRAINING   Loss: 0.5566   Accuracy: 0.999 | VALIDATION   Loss: 0.5465   Accuracy: 0.999 | LR 0.000031\n",
      "Epoch 060 | TRAINING   Loss: 0.5565   Accuracy: 0.999 | VALIDATION   Loss: 0.5469   Accuracy: 0.999 | LR 0.000031\n",
      "\n",
      "Done. Launch TensorBoard with: tensorboard --logdir TensorBoard\n"
     ]
    }
   ],
   "source": [
    "# Training/Validation loop\n",
    "for epoch in range(1, epochs+1):\n",
    "    training_loss, training_accuracy = run_epoch(training_loader, training=True)\n",
    "    validation_loss, validation_accuracy = run_epoch(validation_loader)\n",
    "    \n",
    "    scheduler.step(validation_loss)  # ReduceLROnPlateau on validation loss\n",
    "    learning_rate = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Logs\n",
    "    writer.add_scalar(\"Loss/train\", training_loss,       epoch)\n",
    "    writer.add_scalar(\"Loss/val\",   validation_loss,     epoch)\n",
    "    writer.add_scalar(\"Acc/train\",  training_accuracy,   epoch)\n",
    "    writer.add_scalar(\"Acc/val\",    validation_accuracy, epoch)\n",
    "    writer.add_scalar(\"LR\",         learning_rate,       epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch:03d} | \"\n",
    "          f\"TRAINING   Loss: {training_loss:.4f}   Accuracy: {training_accuracy:.3f} | \"\n",
    "          f\"VALIDATION   Loss: {validation_loss:.4f}   Accuracy: {validation_accuracy:.3f} | \"\n",
    "          f\"LR {learning_rate:.6f}\")\n",
    "    \n",
    "writer.close()\n",
    "print(\"\\nDone. Launch TensorBoard with: tensorboard --logdir TensorBoard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fab7290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30258), started 0:00:10 ago. (Use '!kill 30258' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7f9569f8cb3ade70\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7f9569f8cb3ade70\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f931c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/media/rafael/HD/ml/GraphNeuralNetwork.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bafc501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /media/rafael/HD/ml/GraphNeuralNetwork.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model to a file\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e0228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
